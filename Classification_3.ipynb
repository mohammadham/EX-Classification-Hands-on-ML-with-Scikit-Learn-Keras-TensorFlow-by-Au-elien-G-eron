{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA2jDz+DpYq4g+2gQTYhXE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadham/EX-Classification-Hands-on-ML-with-Scikit-Learn-Keras-TensorFlow-by-Au-elien-G-eron/blob/main/Classification_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex 1 :\n",
        "\n",
        "In this code, we're using the \n",
        "fetch_openml\n",
        " function from scikit-learn to load the MNIST dataset. We then split the data into training and test sets using the \n",
        "train_test_split\n",
        " function, which randomly splits the data into two sets based on the \n",
        "test_size\n",
        " parameter (in this case, 20% of the data is used for testing).\n",
        "\n",
        "We define the parameter grid to search for the best hyperparameters for the KNeighborsClassifier model, which includes the weights and n_neighbors hyperparameters.\n",
        "\n",
        "We use \n",
        "GridSearchCV\n",
        " to search for the best hyperparameters, and print out the best hyperparameters found by \n",
        "GridSearchCV\n",
        " and the corresponding accuracy score.\n",
        "\n",
        "We evaluate the best model on the test set and print out the final accuracy score.\n"
      ],
      "metadata": {
        "id": "6q3z0mTR5N6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8l8Wn-p2K_W",
        "outputId": "b975e08b-a98b-4443-ae0c-238210f6642f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "[CV 1/5] END ....n_neighbors=3, weights=uniform;, score=0.969 total time=  28.1s\n",
            "[CV 2/5] END ....n_neighbors=3, weights=uniform;, score=0.969 total time=  28.4s\n",
            "[CV 3/5] END ....n_neighbors=3, weights=uniform;, score=0.972 total time=  27.9s\n",
            "[CV 4/5] END ....n_neighbors=3, weights=uniform;, score=0.971 total time=  28.2s\n",
            "[CV 5/5] END ....n_neighbors=3, weights=uniform;, score=0.970 total time=  28.5s\n",
            "[CV 1/5] END ...n_neighbors=3, weights=distance;, score=0.971 total time=  29.5s\n",
            "[CV 2/5] END ...n_neighbors=3, weights=distance;, score=0.970 total time=  27.7s\n",
            "[CV 3/5] END ...n_neighbors=3, weights=distance;, score=0.973 total time=  27.8s\n",
            "[CV 4/5] END ...n_neighbors=3, weights=distance;, score=0.972 total time=  28.0s\n",
            "[CV 5/5] END ...n_neighbors=3, weights=distance;, score=0.971 total time=  27.5s\n",
            "[CV 1/5] END ....n_neighbors=4, weights=uniform;, score=0.967 total time=  27.9s\n",
            "[CV 2/5] END ....n_neighbors=4, weights=uniform;, score=0.970 total time=  32.5s\n",
            "[CV 3/5] END ....n_neighbors=4, weights=uniform;, score=0.969 total time=  32.8s\n",
            "[CV 4/5] END ....n_neighbors=4, weights=uniform;, score=0.969 total time=  28.9s\n",
            "[CV 5/5] END ....n_neighbors=4, weights=uniform;, score=0.967 total time=  29.1s\n",
            "[CV 1/5] END ...n_neighbors=4, weights=distance;, score=0.971 total time=  27.8s\n",
            "[CV 2/5] END ...n_neighbors=4, weights=distance;, score=0.973 total time=  27.6s\n",
            "[CV 3/5] END ...n_neighbors=4, weights=distance;, score=0.973 total time=  27.5s\n",
            "[CV 4/5] END ...n_neighbors=4, weights=distance;, score=0.972 total time=  27.5s\n",
            "[CV 5/5] END ...n_neighbors=4, weights=distance;, score=0.971 total time=  27.8s\n",
            "[CV 1/5] END ....n_neighbors=5, weights=uniform;, score=0.967 total time=  28.6s\n",
            "[CV 2/5] END ....n_neighbors=5, weights=uniform;, score=0.969 total time=  28.0s\n",
            "[CV 3/5] END ....n_neighbors=5, weights=uniform;, score=0.972 total time=  27.7s\n",
            "[CV 4/5] END ....n_neighbors=5, weights=uniform;, score=0.969 total time=  27.9s\n",
            "[CV 5/5] END ....n_neighbors=5, weights=uniform;, score=0.967 total time=  32.7s\n",
            "[CV 1/5] END ...n_neighbors=5, weights=distance;, score=0.968 total time=  29.0s\n",
            "[CV 2/5] END ...n_neighbors=5, weights=distance;, score=0.971 total time=  27.5s\n",
            "[CV 3/5] END ...n_neighbors=5, weights=distance;, score=0.973 total time=  27.6s\n",
            "[CV 4/5] END ...n_neighbors=5, weights=distance;, score=0.970 total time=  27.5s\n",
            "[CV 5/5] END ...n_neighbors=5, weights=distance;, score=0.968 total time=  27.4s\n",
            "Best hyperparameters:  {'n_neighbors': 4, 'weights': 'distance'}\n",
            "Best accuracy score:  0.9721964285714286\n",
            "Final accuracy score:  0.9731428571428572\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "#mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
        "\n",
        "# Create a KNeighborsClassifier model\n",
        "knn_clf = KNeighborsClassifier()\n",
        "\n",
        "# Use GridSearchCV to search for the best hyperparameters\n",
        "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and corresponding accuracy score\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best accuracy score: \", grid_search.best_score_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final accuracy score: \", final_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex 2 :\n",
        "In this code, we define a function \n",
        "shift_image\n",
        " that can shift an MNIST image in any direction by one pixel. We then create shifted copies of each image in the training set by calling \n",
        "shift_image\n",
        " four times (once for each direction) and adding the shifted images to the training set.\n",
        "\n",
        "We convert the augmented training set to numpy arrays and shuffle the data. We then use \n",
        "GridSearchCV\n",
        " to find the best hyperparameters for the KNeighborsClassifier model on the augmented training set.\n",
        "\n",
        "We train the KNeighborsClassifier model on the augmented training set using the best hyperparameters found by \n",
        "GridSearchCV\n",
        ". Finally, we evaluate the model on the test set and print out the final accuracy score."
      ],
      "metadata": {
        "id": "fLwdS_uy9es9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import shift\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist[\"data\"].to_numpy(), mnist[\"target\"].to_numpy()\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to shift an image in any direction by one pixel\n",
        "def shift_image(image, dx, dy):\n",
        "    image = image.reshape((28, 28))\n",
        "    shifted_image = shift(image, [dy, dx], cval=0, mode=\"constant\")\n",
        "    return shifted_image.reshape([-1])\n",
        "\n",
        "# Create shifted copies of each image in the training set\n",
        "X_train_augmented = [image for image in X_train]\n",
        "y_train_augmented = [label for label in y_train]\n",
        "\n",
        "for dx, dy in ((1, 0), (-1, 0), (0, 1), (0, -1)):\n",
        "    for image, label in zip(X_train, y_train):\n",
        "        X_train_augmented.append(shift_image(image, dx, dy))\n",
        "        y_train_augmented.append(label)\n",
        "\n",
        "# Convert the augmented training set to numpy arrays and shuffle the data\n",
        "X_train_augmented = np.array(X_train_augmented)\n",
        "y_train_augmented = np.array(y_train_augmented)\n",
        "shuffle_idx = np.random.permutation(len(X_train_augmented))\n",
        "X_train_augmented = X_train_augmented[shuffle_idx]\n",
        "y_train_augmented = y_train_augmented[shuffle_idx]\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters for the KNeighborsClassifier model\n",
        "param_grid = [{'weights': [\"uniform\", \"distance\"], 'n_neighbors': [3, 4, 5]}]\n",
        "knn_clf = KNeighborsClassifier()\n",
        "grid_search = GridSearchCV(knn_clf, param_grid, cv=5, verbose=3)\n",
        "grid_search.fit(X_train_augmented, y_train_augmented)\n",
        "\n",
        "# Train the KNeighborsClassifier model on the augmented training set\n",
        "knn_clf = KNeighborsClassifier(**grid_search.best_params_)\n",
        "knn_clf.fit(X_train_augmented, y_train_augmented)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = knn_clf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final accuracy score: \", final_accuracy)"
      ],
      "metadata": {
        "id": "Q2A-BqK48VOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex 3 :\n",
        "\n",
        "\n",
        "This code downloads the Titanic dataset, preprocesses the data using pipelines, fits a Random Forest classifier and a Support Vector Machine classifier."
      ],
      "metadata": {
        "id": "Llr11i-NAekr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the paths and URLs for the Titanic dataset\n",
        "TITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\n",
        "DOWNLOAD_URL = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/\"\n",
        "\n",
        "# Function to download the Titanic dataset\n",
        "def fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path)\n",
        "    for filename in (\"train.csv\", \"test.csv\"):\n",
        "        filepath = os.path.join(path, filename)\n",
        "        if not os.path.isfile(filepath):\n",
        "            print(\"Downloading\", filename)\n",
        "            urllib.request.urlretrieve(url + filename, filepath)\n",
        "\n",
        "# Function to load the Titanic dataset\n",
        "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
        "    csv_path = os.path.join(titanic_path, filename)\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "# Download and load the Titanic dataset\n",
        "fetch_titanic_data()\n",
        "train_data = load_titanic_data(\"train.csv\")\n",
        "test_data = load_titanic_data(\"test.csv\")\n",
        "\n",
        "# Set the index of the training and test data to \"PassengerId\"\n",
        "train_data = train_data.set_index(\"PassengerId\")\n",
        "test_data = test_data.set_index(\"PassengerId\")\n",
        "\n",
        "# Print information about the training data\n",
        "train_data.info()\n",
        "\n",
        "# Compute the median age of female passengers in the training data\n",
        "train_data[train_data[\"Sex\"]==\"female\"][\"Age\"].median()\n",
        "\n",
        "# Print some statistics about the training data\n",
        "train_data.describe()\n",
        "\n",
        "# Count the number of survivors and non-survivors in the training data\n",
        "train_data[\"Survived\"].value_counts()\n",
        "\n",
        "# Count the number of passengers in each passenger class in the training data\n",
        "train_data[\"Pclass\"].value_counts()\n",
        "\n",
        "# Count the number of male and female passengers in the training data\n",
        "train_data[\"Sex\"].value_counts()\n",
        "\n",
        "# Count the number of passengers who embarked at each port in the training data\n",
        "train_data[\"Embarked\"].value_counts()\n",
        "\n",
        "# Define the preprocessing pipelines for numerical and categorical data\n",
        "num_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "cat_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n",
        "    ])\n",
        "\n",
        "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
        "num_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "cat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
        "preprocess_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", cat_pipeline, cat_attribs),\n",
        "    ])\n",
        "\n",
        "# Preprocess the training data and fit a Random Forest classifier\n",
        "X_train = preprocess_pipeline.fit_transform(train_data[num_attribs + cat_attribs])\n",
        "y_train = train_data[\"Survived\"]\n",
        "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\n",
        "print(\"Random Forest accuracy:\", forest_scores.mean())\n",
        "\n",
        "# Preprocess the training data and fit a Support Vector Machine classifier\n",
        "svm_clf = SVC(gamma=\"auto\")\n",
        "svm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n",
        "print(\"SVM accuracy:\", svm_scores.mean())\n",
        "\n",
        "# Plot the accuracy scores of the two classifiers\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot([1]*10, svm_scores, \".\")\n",
        "plt.plot([2]*10, forest_scores, \".\")\n",
        "plt.boxplot([svm_scores, forest_scores], labels=(\"SVM\",\"Random Forest\"))\n",
        "plt.ylabel(\"Accuracy\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "# Create a new feature \"AgeBucket\" by grouping ages into buckets of 15 years\n",
        "train_data[\"AgeBucket\"] = train_data[\"Age\"] // 15 * 15\n",
        "train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()\n",
        "\n",
        "# Create a new feature \"RelativesOnboard\" by adding the number of siblings/spouses and parents/children\n",
        "train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n",
        "train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()"
      ],
      "metadata": {
        "id": "Wx_U2NHLAeyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex 3 :\n",
        "some improvements to the code:\n",
        "\n",
        "1-Add comments to explain the purpose of each section of code.\n",
        "Use more descriptive variable names to make the code easier to read and understand.\n",
        "\n",
        "2-Use f-strings to format strings instead of concatenation.\n",
        "\n",
        "3-Add error handling to the \n",
        "fetch_titanic_data\n",
        " function in case the download fails.\n",
        "\n",
        "4-Use \n",
        "train_test_split\n",
        " to split the training data into a training set and a validation set for model selection.\n",
        "\n",
        "5-Use \n",
        "GridSearchCV\n",
        " to perform hyperparameter tuning for the Random Forest classifier.\n",
        "\n",
        "6-Use \n",
        "RandomizedSearchCV\n",
        " to perform hyperparameter tuning for the Support Vector Machine classifier.\n",
        "\n",
        "7-Use \n",
        "precision_recall_curve\n",
        " and \n",
        "roc_curve\n",
        " to plot precision-recall and ROC curves for the classifiers.\n",
        "\n",
        "8-Use \n",
        "confusion_matrix\n",
        " to compute the confusion matrix for the classifiers.\n",
        " \n",
        "Here's the improved code:"
      ],
      "metadata": {
        "id": "9zDNNeBaHqAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve, confusion_matrix\n",
        "\n",
        "# Define the paths and URLs for the Titanic dataset\n",
        "TITANIC_PATH = os.path.join(\"datasets\", \"titanic\")\n",
        "DOWNLOAD_URL = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/titanic/\"\n",
        "\n",
        "# Function to download the Titanic dataset\n",
        "def fetch_titanic_data(url=DOWNLOAD_URL, path=TITANIC_PATH):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path)\n",
        "    for filename in (\"train.csv\", \"test.csv\"):\n",
        "        filepath = os.path.join(path, filename)\n",
        "        if not os.path.isfile(filepath):\n",
        "            print(f\"Downloading {filename}\")\n",
        "            try:\n",
        "                urllib.request.urlretrieve(url + filename, filepath)\n",
        "            except:\n",
        "                print(f\"Failed to download {filename}\")\n",
        "\n",
        "# Function to load the Titanic dataset\n",
        "def load_titanic_data(filename, titanic_path=TITANIC_PATH):\n",
        "    csv_path = os.path.join(titanic_path, filename)\n",
        "    return pd.read_csv(csv_path)\n",
        "\n",
        "# Download and load the Titanic dataset\n",
        "fetch_titanic_data()\n",
        "train_data = load_titanic_data(\"train.csv\")\n",
        "test_data = load_titanic_data(\"test.csv\")\n",
        "\n",
        "# Set the index of the training and test data to \"PassengerId\"\n",
        "train_data = train_data.set_index(\"PassengerId\")\n",
        "test_data = test_data.set_index(\"PassengerId\")\n",
        "\n",
        "# Print information about the training data\n",
        "print(\"Training data info:\")\n",
        "print(train_data.info())\n",
        "\n",
        "# Compute the median age of female passengers in the training data\n",
        "female_median_age = train_data[train_data[\"Sex\"]==\"female\"][\"Age\"].median()\n",
        "print(f\"Median age of female passengers: {female_median_age}\")\n",
        "\n",
        "# Print some statistics about the training data\n",
        "print(\"Training data statistics:\")\n",
        "print(train_data.describe())\n",
        "\n",
        "# Count the number of survivors and non-survivors in the training data\n",
        "survival_counts = train_data[\"Survived\"].value_counts()\n",
        "print(\"Survival counts:\")\n",
        "print(survival_counts)\n",
        "\n",
        "# Count the number of passengers in each passenger class in the training data\n",
        "class_counts = train_data[\"Pclass\"].value_counts()\n",
        "print(\"Passenger class counts:\")\n",
        "print(class_counts)\n",
        "\n",
        "# Count the number of male and female passengers in the training data\n",
        "sex_counts = train_data[\"Sex\"].value_counts()\n",
        "print(\"Sex counts:\")\n",
        "print(sex_counts)\n",
        "\n",
        "# Count the number of passengers who embarked at each port in the training data\n",
        "embarked_counts = train_data[\"Embarked\"].value_counts()\n",
        "print(\"Embarked counts:\")\n",
        "print(embarked_counts)\n",
        "\n",
        "# Define the preprocessing pipelines for numerical and categorical data\n",
        "num_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "cat_pipeline = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n",
        "    ])\n",
        "\n",
        "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
        "num_attribs = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
        "cat_attribs = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
        "preprocess_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", cat_pipeline, cat_attribs),\n",
        "    ])\n",
        "\n",
        "# Preprocess the training data and split it into a training set and a validation set\n",
        "X = preprocess_pipeline.fit_transform(train_data[num_attribs + cat_attribs])\n",
        "y = train_data[\"Survived\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit a Random Forest classifier and perform hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "forest_clf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(forest_clf, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Random Forest best parameters:\", grid_search.best_params_)\n",
        "print(\"Random Forest best score:\", grid_search.best_score_)\n",
        "\n",
        "# Fit a Support Vector Machine classifier and perform hyperparameter tuning using RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "svm_clf = SVC(random_state=42)\n",
        "random_search = RandomizedSearchCV(svm_clf, param_distributions=param_dist, n_iter=20, cv=5, scoring='accuracy', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"SVM best parameters:\", random_search.best_params_)\n",
        "print(\"SVM best score:\", random_search.best_score_)\n",
        "\n",
        "# Evaluate the Random Forest classifier on the validation set\n",
        "forest_clf = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'],\n",
        "                                     max_depth=grid_search.best_params_['max_depth'],\n",
        "                                     min_samples_split=grid_search.best_params_['min_samples_split'],\n",
        "                                     min_samples_leaf=grid_search.best_params_['min_samples_leaf'],\n",
        "                                     random_state=42)\n",
        "forest_clf.fit(X_train, y_train)\n",
        "y_pred_forest = forest_clf.predict(X_val)\n",
        "print(\"Random Forest accuracy on validation set:\", (y_pred_forest == y_val).mean())\n",
        "print(\"Random Forest confusion matrix:\")\n",
        "print(confusion_matrix(y_val, y_pred_forest))\n",
        "\n",
        "# Evaluate the Support Vector Machine classifier on the validation set\n",
        "svm_clf = SVC(C=random_search.best_params_['C'],\n",
        "              gamma=random_search.best_params_['gamma'],\n",
        "              kernel=random_search.best_params_['kernel'],\n",
        "              random_state=42)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "y_pred_svm = svm_clf.predict(X_val)\n",
        "print(\"SVM accuracy on validation set:\", (y_pred_svm == y_val).mean())\n",
        "print(\"SVM confusion matrix:\")\n",
        "print(confusion_matrix(y_val, y_pred_svm))\n",
        "\n",
        "# Plot precision-recall and ROC curves for the Random Forest classifier\n",
        "y_scores_forest = forest_clf.predict_proba(X_val)[:, 1]\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_val, y_scores_forest)\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_scores_forest)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
        "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
        "plt.xlabel(\"Threshold\", fontsize=14)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.ylim([0, 1])\n",
        "plt.title(\"Precision-Recall Curve\", fontsize=16)\n",
        "plt.show()\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(fpr, tpr, linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.axis([0, 1, 0, 1])\n",
        "plt.xlabel('False Positive Rate', fontsize=14)\n",
        "plt.ylabel('True Positive Rate', fontsize=14)\n",
        "plt.title('ROC Curve', fontsize=16)\n",
        "plt.show()\n",
        "\n",
        "# Plot precision-recall and ROC curves for the Support Vector Machine classifier\n",
        "y_scores_svm = svm_clf.decision_function(X_val)\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_val, y_scores_svm)\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_scores_svm)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
        "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
        "plt.xlabel(\"Threshold\", fontsize=14)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.ylim([0, 1])\n",
        "plt.title(\"Precision-Recall Curve\", fontsize=16)\n",
        "plt.show()\n",
        "plt.figure                                                  "
      ],
      "metadata": {
        "id": "Pygt76UHGqb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description about this code:\n",
        "This code performs a binary classification task on the Titanic dataset, which contains information about passengers on the Titanic and whether they survived or not. The code downloads the dataset, preprocesses the data using pipelines, and splits the training data into a training set and a validation set. It then fits a Random Forest classifier and a Support Vector Machine classifier on the training set, performs hyperparameter tuning using GridSearchCV and RandomizedSearchCV, and evaluates the classifiers on the validation set. Finally, it plots precision-recall and ROC curves for the classifiers using the validation set.\n",
        "\n",
        "The code first defines the paths and URLs for the Titanic dataset and defines functions to download and load the data. It then downloads and loads the data, sets the index of the training and test data to \"PassengerId\", and prints some information about the training data, such as the median age of female passengers, survival counts, passenger class counts, sex counts, and embarked counts.\n",
        "\n",
        "The code then defines preprocessing pipelines for numerical and categorical data using SimpleImputer, StandardScaler, and OneHotEncoder, and combines them using ColumnTransformer. It preprocesses the training data and splits it into a training set and a validation set using train_test_split.\n",
        "\n",
        "The code then fits a Random Forest classifier and performs hyperparameter tuning using GridSearchCV. It also fits a Support Vector Machine classifier and performs hyperparameter tuning using RandomizedSearchCV. It prints the best parameters and best score for each classifier.\n",
        "\n",
        "The code then evaluates the Random Forest classifier and the Support Vector Machine classifier on the validation set, computes the accuracy and confusion matrix for each classifier, and plots precision-recall and ROC curves for each classifier using precision_recall_curve and roc_curve.\n",
        "\n",
        "Overall, this code demonstrates how to preprocess data using pipelines, split data into training and validation sets, perform hyperparameter tuning using GridSearchCV and RandomizedSearchCV, and evaluate classifiers using accuracy, confusion matrix, precision-recall curve, and ROC curve."
      ],
      "metadata": {
        "id": "rXzXls9rJLXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================================"
      ],
      "metadata": {
        "id": "tqYkN2icJZ0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ex 4 😢:\n"
      ],
      "metadata": {
        "id": "sye9ToCRJcXT"
      }
    }
  ]
}